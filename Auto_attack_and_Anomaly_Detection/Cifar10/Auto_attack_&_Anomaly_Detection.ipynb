{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5bJB-CIYD3xE"
   },
   "source": [
    "Before starting we need to import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "MjoLUW9aCofP"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import argparse\n",
    "import sys\n",
    "sys.argv\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YAvCsdRbCdJQ"
   },
   "source": [
    "To ensure we get reproducible results we set the random seed for Python, Numpy and PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "BN4WxssWDLJN"
   },
   "outputs": [],
   "source": [
    "SEED=1234\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic=True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='PyTorch CIFAR 10 Training')\n",
    "parser.add_argument('--batch-size', type=int, default=128, metavar='N',\n",
    "                    help='input batch size for training (default: 128)')\n",
    "parser.add_argument('--test-batch-size', type=int, default=128, metavar='N',\n",
    "                    help='input batch size for testing (default: 128)')\n",
    "parser.add_argument('--epochs', type=int, default=76, metavar='N',\n",
    "                    help='number of epochs to train')\n",
    "parser.add_argument('--weight-decay', '--wd', default=2e-4,\n",
    "                    type=float, metavar='W')\n",
    "parser.add_argument('--lr', type=float, default=0.1, metavar='LR',\n",
    "                    help='learning rate')\n",
    "parser.add_argument('--momentum', type=float, default=0.9, metavar='M',\n",
    "                    help='SGD momentum')\n",
    "parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                    help='disables CUDA training')\n",
    "parser.add_argument('--epsilon', default=0.031,\n",
    "                    help='perturbation')\n",
    "parser.add_argument('--num-steps', default=10,\n",
    "                    help='perturb number of steps')\n",
    "parser.add_argument('--step-size', default=0.007,\n",
    "                    help='perturb step size')\n",
    "parser.add_argument('--beta', default=6.0,\n",
    "                    help='regularization, i.e., 1/lambda in TRADES')\n",
    "parser.add_argument('--seed', type=int, default=1, metavar='S',\n",
    "                    help='random seed (default: 1)')\n",
    "parser.add_argument('--log-interval', type=int, default=100, metavar='N',\n",
    "                    help='how many batches to wait before logging training status')\n",
    "parser.add_argument('--model-dir', default='./model-cifar-wideResNet',\n",
    "                    help='directory of model for saving checkpoint')\n",
    "parser.add_argument('--save-freq', '-s', default=1, type=int, metavar='N',\n",
    "                    help='save frequency')\n",
    "\n",
    "args, unknown = parser.parse_known_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion * planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion * planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, self.expansion * planes, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(self.expansion * planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion * planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion * planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=100):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def ResNet18():\n",
    "    return ResNet(BasicBlock, [2, 2, 2, 2])\n",
    "def test():\n",
    "    net = ResNet18()\n",
    "    y = net(torch.randn(1, 3, 32, 32))\n",
    "    print(y.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "torch.manual_seed(args.seed)\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7QPDKtcWSPop"
   },
   "source": [
    "1.  first we need to load the last checkpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "im4jWiRDO0hM",
    "outputId": "f259735c-c0c3-408f-b871-0a16a41a9727"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (linear): Linear(in_features=512, out_features=100, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = ResNet18()\n",
    "net.load_state_dict(torch.load('model-resnet-epoch76.pt'))\n",
    "net.to(device) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "itKdNAobN8IF"
   },
   "source": [
    "2.  Applying Auto attack to dataset and evaluate our trained model:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9U2cAD5ZOVaN"
   },
   "source": [
    "*  in order to apply auto attack, we need to get all of required packages from Auto Attack github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n0CWCTzOObqk",
    "outputId": "3255b2c4-34fb-4b77-f1d9-2167f528d196"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/fra31/auto-attack\n",
      "  Cloning https://github.com/fra31/auto-attack to c:\\users\\zahra\\appdata\\local\\temp\\pip-req-build-th880dno\n",
      "  Resolved https://github.com/fra31/auto-attack to commit a39220048b3c9f2cca9a4d3a54604793c68eca7e\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet https://github.com/fra31/auto-attack 'C:\\Users\\zahra\\AppData\\Local\\Temp\\pip-req-build-th880dno'\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/fra31/auto-attack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WPbv5NCTQfFd"
   },
   "source": [
    "*  Applying AA attack for Linf setting and calculating Adversarial attack rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9F9ygv57c1OF",
    "outputId": "36567e0a-f246-48f4-e400-58da591f2f21"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting parameters for standard version\n",
      "Files already downloaded and verified\n",
      "using standard version including apgd-ce, apgd-t, fab-t, square.\n",
      "initial accuracy: 92.76%\n",
      "apgd-ce - 1/19 - 500 out of 500 successfully perturbed\n",
      "apgd-ce - 2/19 - 498 out of 500 successfully perturbed\n",
      "apgd-ce - 3/19 - 500 out of 500 successfully perturbed\n",
      "apgd-ce - 4/19 - 500 out of 500 successfully perturbed\n",
      "apgd-ce - 5/19 - 499 out of 500 successfully perturbed\n",
      "apgd-ce - 6/19 - 500 out of 500 successfully perturbed\n",
      "apgd-ce - 7/19 - 499 out of 500 successfully perturbed\n",
      "apgd-ce - 8/19 - 500 out of 500 successfully perturbed\n",
      "apgd-ce - 9/19 - 500 out of 500 successfully perturbed\n",
      "apgd-ce - 10/19 - 498 out of 500 successfully perturbed\n",
      "apgd-ce - 11/19 - 499 out of 500 successfully perturbed\n",
      "apgd-ce - 12/19 - 500 out of 500 successfully perturbed\n",
      "apgd-ce - 13/19 - 500 out of 500 successfully perturbed\n",
      "apgd-ce - 14/19 - 500 out of 500 successfully perturbed\n",
      "apgd-ce - 15/19 - 496 out of 500 successfully perturbed\n",
      "apgd-ce - 16/19 - 497 out of 500 successfully perturbed\n",
      "apgd-ce - 17/19 - 498 out of 500 successfully perturbed\n",
      "apgd-ce - 18/19 - 498 out of 500 successfully perturbed\n",
      "apgd-ce - 19/19 - 276 out of 276 successfully perturbed\n",
      "robust accuracy after APGD-CE: 0.18% (total time 305.1 s)\n",
      "apgd-t - 1/1 - 16 out of 18 successfully perturbed\n",
      "robust accuracy after APGD-T: 0.02% (total time 306.1 s)\n",
      "fab-t - 1/1 - 2 out of 2 successfully perturbed\n",
      "robust accuracy after FAB-T: 0.00% (total time 306.1 s)\n",
      "max Linf perturbation: 0.03100, nan in tensor: 0, max: 1.00000, min: 0.00000\n",
      "robust accuracy: 0.00%\n"
     ]
    }
   ],
   "source": [
    "from autoattack import AutoAttack\n",
    "adversary = AutoAttack(net, norm='Linf', eps=0.031, version='standard')\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "test_loader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)\n",
    "l = [x for (x, y) in test_loader]\n",
    "x_test = torch.cat(l, 0)\n",
    "l = [y for (x, y) in test_loader]\n",
    "y_test = torch.cat(l, 0)\n",
    "adv_complete = adversary.run_standard_evaluation(x_test, y_test,bs=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "37s13pZw5BeI",
    "outputId": "da0e2abd-bd7b-4422-e7af-4a9d4b3b2011"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adversarial attack rate for Linf norm: 90.02%\n"
     ]
    }
   ],
   "source": [
    "adv_loader_Linf = torch.utils.data.DataLoader(adv_complete, batch_size=100, shuffle=False, num_workers=2)\n",
    "num_total_images = 0\n",
    "num_successful_attacks = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "    for adv_images in adv_loader_Linf:\n",
    "        adv_images = adv_images.to(device)\n",
    "        outputs_adv = net(adv_images)\n",
    "        _, predicted_adv = torch.max(outputs_adv.data, 1)\n",
    "\n",
    "        # Count the number of adversarial examples that were successfully attacked\n",
    "        for i in range(len(images)):\n",
    "            if predicted[i] != predicted_adv[i]:\n",
    "                num_successful_attacks += 1\n",
    "        \n",
    "        # Increment the total number of images\n",
    "        num_total_images += len(images)\n",
    "\n",
    "# Calculate the adversarial attack rate\n",
    "attack_rate = (num_successful_attacks / num_total_images) * 100\n",
    "print(f\"Adversarial attack rate for Linf norm: {attack_rate:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "idDPPMhGaJeN"
   },
   "source": [
    "*  Applying AA attack for L2 setting and calculating Adversarial attack rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "lyyvRSsiaIm_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting parameters for standard version\n",
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data\\cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████| 170498071/170498071 [00:09<00:00, 17305109.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\cifar-10-python.tar.gz to ./data\n",
      "using standard version including apgd-ce, apgd-t, fab-t, square.\n",
      "initial accuracy: 92.76%\n",
      "apgd-ce - 1/19 - 52 out of 500 successfully perturbed\n",
      "apgd-ce - 2/19 - 40 out of 500 successfully perturbed\n",
      "apgd-ce - 3/19 - 42 out of 500 successfully perturbed\n",
      "apgd-ce - 4/19 - 50 out of 500 successfully perturbed\n",
      "apgd-ce - 5/19 - 63 out of 500 successfully perturbed\n",
      "apgd-ce - 6/19 - 43 out of 500 successfully perturbed\n",
      "apgd-ce - 7/19 - 49 out of 500 successfully perturbed\n",
      "apgd-ce - 8/19 - 41 out of 500 successfully perturbed\n",
      "apgd-ce - 9/19 - 45 out of 500 successfully perturbed\n",
      "apgd-ce - 10/19 - 53 out of 500 successfully perturbed\n",
      "apgd-ce - 11/19 - 59 out of 500 successfully perturbed\n",
      "apgd-ce - 12/19 - 55 out of 500 successfully perturbed\n",
      "apgd-ce - 13/19 - 48 out of 500 successfully perturbed\n",
      "apgd-ce - 14/19 - 40 out of 500 successfully perturbed\n",
      "apgd-ce - 15/19 - 45 out of 500 successfully perturbed\n",
      "apgd-ce - 16/19 - 54 out of 500 successfully perturbed\n",
      "apgd-ce - 17/19 - 57 out of 500 successfully perturbed\n",
      "apgd-ce - 18/19 - 39 out of 500 successfully perturbed\n",
      "apgd-ce - 19/19 - 16 out of 276 successfully perturbed\n",
      "robust accuracy after APGD-CE: 83.85% (total time 304.3 s)\n",
      "apgd-t - 1/17 - 6 out of 500 successfully perturbed\n",
      "apgd-t - 2/17 - 5 out of 500 successfully perturbed\n",
      "apgd-t - 3/17 - 4 out of 500 successfully perturbed\n",
      "apgd-t - 4/17 - 6 out of 500 successfully perturbed\n",
      "apgd-t - 5/17 - 3 out of 500 successfully perturbed\n",
      "apgd-t - 6/17 - 8 out of 500 successfully perturbed\n",
      "apgd-t - 7/17 - 3 out of 500 successfully perturbed\n",
      "apgd-t - 8/17 - 6 out of 500 successfully perturbed\n",
      "apgd-t - 9/17 - 8 out of 500 successfully perturbed\n",
      "apgd-t - 10/17 - 5 out of 500 successfully perturbed\n",
      "apgd-t - 11/17 - 4 out of 500 successfully perturbed\n",
      "apgd-t - 12/17 - 8 out of 500 successfully perturbed\n",
      "apgd-t - 13/17 - 2 out of 500 successfully perturbed\n",
      "apgd-t - 14/17 - 3 out of 500 successfully perturbed\n",
      "apgd-t - 15/17 - 6 out of 500 successfully perturbed\n",
      "apgd-t - 16/17 - 6 out of 500 successfully perturbed\n",
      "apgd-t - 17/17 - 3 out of 385 successfully perturbed\n",
      "robust accuracy after APGD-T: 82.99% (total time 2780.7 s)\n",
      "fab-t - 1/17 - 0 out of 500 successfully perturbed\n",
      "fab-t - 2/17 - 0 out of 500 successfully perturbed\n",
      "fab-t - 3/17 - 0 out of 500 successfully perturbed\n",
      "fab-t - 4/17 - 0 out of 500 successfully perturbed\n",
      "fab-t - 5/17 - 0 out of 500 successfully perturbed\n",
      "fab-t - 6/17 - 0 out of 500 successfully perturbed\n",
      "fab-t - 7/17 - 1 out of 500 successfully perturbed\n",
      "fab-t - 8/17 - 0 out of 500 successfully perturbed\n",
      "fab-t - 9/17 - 0 out of 500 successfully perturbed\n",
      "fab-t - 10/17 - 0 out of 500 successfully perturbed\n",
      "fab-t - 11/17 - 0 out of 500 successfully perturbed\n",
      "fab-t - 12/17 - 0 out of 500 successfully perturbed\n",
      "fab-t - 13/17 - 1 out of 500 successfully perturbed\n",
      "fab-t - 14/17 - 0 out of 500 successfully perturbed\n",
      "fab-t - 15/17 - 0 out of 500 successfully perturbed\n",
      "fab-t - 16/17 - 0 out of 500 successfully perturbed\n",
      "fab-t - 17/17 - 0 out of 299 successfully perturbed\n",
      "robust accuracy after FAB-T: 82.97% (total time 6997.5 s)\n",
      "square - 1/17 - 0 out of 500 successfully perturbed\n",
      "square - 2/17 - 0 out of 500 successfully perturbed\n",
      "square - 3/17 - 0 out of 500 successfully perturbed\n",
      "square - 4/17 - 0 out of 500 successfully perturbed\n",
      "square - 5/17 - 0 out of 500 successfully perturbed\n",
      "square - 6/17 - 0 out of 500 successfully perturbed\n",
      "square - 7/17 - 0 out of 500 successfully perturbed\n",
      "square - 8/17 - 0 out of 500 successfully perturbed\n",
      "square - 9/17 - 0 out of 500 successfully perturbed\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m l \u001b[38;5;241m=\u001b[39m [y \u001b[38;5;28;01mfor\u001b[39;00m (x, y) \u001b[38;5;129;01min\u001b[39;00m test_loader]\n\u001b[0;32m     11\u001b[0m y_test \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(l, \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m---> 12\u001b[0m adv_complete_L2 \u001b[38;5;241m=\u001b[39m \u001b[43madversary\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_standard_evaluation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43mbs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\autoattack\\autoattack.py:204\u001b[0m, in \u001b[0;36mAutoAttack.run_standard_evaluation\u001b[1;34m(self, x_orig, y_orig, bs, return_labels, state_path)\u001b[0m\n\u001b[0;32m    201\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m attack \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msquare\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    202\u001b[0m     \u001b[38;5;66;03m# square\u001b[39;00m\n\u001b[0;32m    203\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msquare\u001b[38;5;241m.\u001b[39mseed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_seed()\n\u001b[1;32m--> 204\u001b[0m     adv_curr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msquare\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mperturb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m attack \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mapgd-t\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;66;03m# targeted apgd\u001b[39;00m\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapgd_targeted\u001b[38;5;241m.\u001b[39mseed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_seed()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\autoattack\\square.py:595\u001b[0m, in \u001b[0;36mSquareAttack.perturb\u001b[1;34m(self, x, y)\u001b[0m\n\u001b[0;32m    592\u001b[0m x_to_fool \u001b[38;5;241m=\u001b[39m x[ind_to_fool]\u001b[38;5;241m.\u001b[39mclone()\n\u001b[0;32m    593\u001b[0m y_to_fool \u001b[38;5;241m=\u001b[39m y[ind_to_fool]\u001b[38;5;241m.\u001b[39mclone()\n\u001b[1;32m--> 595\u001b[0m _, adv_curr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattack_single_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_to_fool\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_to_fool\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    597\u001b[0m output_curr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(adv_curr)\n\u001b[0;32m    598\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtargeted:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\autoattack\\square.py:383\u001b[0m, in \u001b[0;36mSquareAttack.attack_single_run\u001b[1;34m(self, x, y)\u001b[0m\n\u001b[0;32m    380\u001b[0m x_new \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_shape(x_new)\n\u001b[0;32m    381\u001b[0m norms_image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlp_norm(x_new \u001b[38;5;241m-\u001b[39m x_curr)\n\u001b[1;32m--> 383\u001b[0m margin, loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmargin_and_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_new\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_curr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    385\u001b[0m \u001b[38;5;66;03m# update loss if new loss is better\u001b[39;00m\n\u001b[0;32m    386\u001b[0m idx_improved \u001b[38;5;241m=\u001b[39m (loss \u001b[38;5;241m<\u001b[39m loss_min_curr)\u001b[38;5;241m.\u001b[39mfloat()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\autoattack\\square.py:76\u001b[0m, in \u001b[0;36mSquareAttack.margin_and_loss\u001b[1;34m(self, x, y)\u001b[0m\n\u001b[0;32m     74\u001b[0m xent \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mcross_entropy(logits, y, reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     75\u001b[0m u \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39marange(x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m---> 76\u001b[0m y_corr \u001b[38;5;241m=\u001b[39m \u001b[43mlogits\u001b[49m\u001b[43m[\u001b[49m\u001b[43mu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     77\u001b[0m logits[u, y] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minf\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     78\u001b[0m y_others \u001b[38;5;241m=\u001b[39m logits\u001b[38;5;241m.\u001b[39mmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from autoattack import AutoAttack\n",
    "adversary = AutoAttack(net, norm='L2', eps=0.031, version='standard')\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "test_loader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)\n",
    "l = [x for (x, y) in test_loader]\n",
    "x_test = torch.cat(l, 0)\n",
    "l = [y for (x, y) in test_loader]\n",
    "y_test = torch.cat(l, 0)\n",
    "adv_complete_L2 = adversary.run_standard_evaluation(x_test, y_test,bs=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_pQTNH8paLzn"
   },
   "outputs": [],
   "source": [
    "adv_loader_L2 = torch.utils.data.DataLoader(adv_complete_L2, batch_size=100, shuffle=False, num_workers=2)\n",
    "num_total_images = 0\n",
    "num_successful_attacks = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "    for adv_images in adv_loader_L2:\n",
    "        adv_images = adv_images.to(device)\n",
    "        outputs_adv = net(adv_images)\n",
    "        _, predicted_adv = torch.max(outputs_adv.data, 1)\n",
    "\n",
    "        # Count the number of adversarial examples that were successfully attacked\n",
    "        for i in range(len(images)):\n",
    "            if predicted[i] != predicted_adv[i]:\n",
    "                num_successful_attacks += 1\n",
    "        \n",
    "        # Increment the total number of images\n",
    "        num_total_images += len(images)\n",
    "\n",
    "# Calculate the adversarial attack rate\n",
    "attack_rate = (num_successful_attacks / num_total_images) * 100\n",
    "print(f\"Adversarial attack rate for L2 norm: {attack_rate:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UGGepO7OMQJR"
   },
   "source": [
    "3.  Anomaly detection:\n",
    "\n",
    "*  to apply our anomally detection algorithm we download the packages from github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5Yc5P5vuMRvD",
    "outputId": "1006ec2d-64b8-4721-e906-02583beddba5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'Mean-Shifted-Anomaly-Detection'...\n",
      "remote: Enumerating objects: 55, done.\u001b[K\n",
      "remote: Counting objects: 100% (9/9), done.\u001b[K\n",
      "remote: Compressing objects: 100% (6/6), done.\u001b[K\n",
      "remote: Total 55 (delta 4), reused 4 (delta 3), pack-reused 46\u001b[K\n",
      "Unpacking objects: 100% (55/55), 17.41 KiB | 1.34 MiB/s, done.\n",
      "/content/Mean-Shifted-Anomaly-Detection\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/talreiss/Mean-Shifted-Anomaly-Detection.git\n",
    "%cd Mean-Shifted-Anomaly-Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "38V8toW-Uh8X"
   },
   "source": [
    "*  we download the required packages and install necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9-ruWCfaVJ8X",
    "outputId": "4aa02ab8-4781-43d7-b9b9-feb7e103a576"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: virtualenv: command not found\n",
      "/bin/bash: venv/bin/activate: No such file or directory\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
      "Collecting faiss-gpu==1.7.1\n",
      "  Downloading faiss_gpu-1.7.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (89.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.7/89.7 MB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 2)) (1.2.0)\n",
      "Requirement already satisfied: numpy>=1.22 in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 3)) (1.22.4)\n",
      "Collecting Pillow>=9.0.1\n",
      "  Downloading Pillow-9.5.0-cp39-cp39-manylinux_2_28_x86_64.whl (3.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m75.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: Could not find a version that satisfies the requirement pkg-resources==0.0.0 (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for pkg-resources==0.0.0\u001b[0m\u001b[31m\n",
      "\u001b[0mLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting faiss_gpu\n",
      "  Downloading faiss_gpu-1.7.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (85.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.5/85.5 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: faiss_gpu\n",
      "Successfully installed faiss_gpu-1.7.2\n"
     ]
    }
   ],
   "source": [
    "!virtualenv venv --python python3\n",
    "!source venv/bin/activate\n",
    "!pip install -r requirements.txt --find-links https://download.pytorch.org/whl/torch_stable.html\n",
    "!pip install faiss_gpu\n",
    "import os\n",
    "\n",
    "os.mkdir('data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1fgogCqHWJCv"
   },
   "source": [
    "*  inorder to train a anomaly detection model for label 0 we can run following code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s09LSrKXVSVA",
    "outputId": "2c9a28a1-f8d4-4572-9ddf-ab0ce4b44d7e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: cifar10, Normal Label: 0, LR: 1e-05\n",
      "cuda:0\n",
      "/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
      "100% 44.7M/44.7M [00:00<00:00, 334MB/s]\n",
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to data/cifar-10-python.tar.gz\n",
      "100% 170498071/170498071 [00:13<00:00, 12727458.52it/s]\n",
      "Extracting data/cifar-10-python.tar.gz to data\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Train set feature extracting: 100% 79/79 [00:17<00:00,  4.63it/s]\n",
      "Test set feature extracting: 100% 157/157 [00:18<00:00,  8.57it/s]\n",
      "Epoch: 0, AUROC is: 0.9433167777777778\n",
      "Train...: 100% 79/79 [01:07<00:00,  1.17it/s]\n",
      "Epoch: 1, Loss: 3.4957426712036135\n",
      "Train set feature extracting: 100% 79/79 [00:12<00:00,  6.42it/s]\n",
      "Test set feature extracting: 100% 157/157 [00:18<00:00,  8.67it/s]\n",
      "Epoch: 1, AUROC is: 0.9478591111111112\n",
      "Train...: 100% 79/79 [01:09<00:00,  1.14it/s]\n",
      "Epoch: 2, Loss: 3.433603253936768\n",
      "Train set feature extracting: 100% 79/79 [00:09<00:00,  8.13it/s]\n",
      "Test set feature extracting: 100% 157/157 [00:17<00:00,  8.87it/s]\n",
      "Epoch: 2, AUROC is: 0.9501251111111111\n",
      "Train...: 100% 79/79 [01:17<00:00,  1.03it/s]\n",
      "Epoch: 3, Loss: 3.361681659126282\n",
      "Train set feature extracting: 100% 79/79 [00:10<00:00,  7.37it/s]\n",
      "Test set feature extracting: 100% 157/157 [00:19<00:00,  7.88it/s]\n",
      "Epoch: 3, AUROC is: 0.9517291111111111\n",
      "Train...: 100% 79/79 [01:09<00:00,  1.13it/s]\n",
      "Epoch: 4, Loss: 3.368151025390625\n",
      "Train set feature extracting: 100% 79/79 [00:11<00:00,  6.99it/s]\n",
      "Test set feature extracting: 100% 157/157 [00:23<00:00,  6.56it/s]\n",
      "Epoch: 4, AUROC is: 0.952574111111111\n",
      "Train...: 100% 79/79 [01:15<00:00,  1.05it/s]\n",
      "Epoch: 5, Loss: 3.329555906677246\n",
      "Train set feature extracting: 100% 79/79 [00:08<00:00,  9.10it/s]\n",
      "Test set feature extracting: 100% 157/157 [00:19<00:00,  7.86it/s]\n",
      "Epoch: 5, AUROC is: 0.9534285\n",
      "Train...: 100% 79/79 [01:09<00:00,  1.14it/s]\n",
      "Epoch: 6, Loss: 3.329956265830994\n",
      "Train set feature extracting: 100% 79/79 [00:09<00:00,  8.17it/s]\n",
      "Test set feature extracting: 100% 157/157 [00:18<00:00,  8.61it/s]\n",
      "Epoch: 6, AUROC is: 0.9542064444444442\n",
      "Train...: 100% 79/79 [01:08<00:00,  1.16it/s]\n",
      "Epoch: 7, Loss: 3.3130846225738524\n",
      "Train set feature extracting: 100% 79/79 [00:08<00:00,  9.13it/s]\n",
      "Test set feature extracting: 100% 157/157 [00:19<00:00,  8.17it/s]\n",
      "Epoch: 7, AUROC is: 0.9549955555555555\n",
      "Train...: 100% 79/79 [01:06<00:00,  1.19it/s]\n",
      "Epoch: 8, Loss: 3.30520853767395\n",
      "Train set feature extracting: 100% 79/79 [00:09<00:00,  8.12it/s]\n",
      "Test set feature extracting: 100% 157/157 [00:18<00:00,  8.69it/s]\n",
      "Epoch: 8, AUROC is: 0.9554685555555555\n",
      "Train...: 100% 79/79 [01:08<00:00,  1.15it/s]\n",
      "Epoch: 9, Loss: 3.2820011432647704\n",
      "Train set feature extracting: 100% 79/79 [00:09<00:00,  8.30it/s]\n",
      "Test set feature extracting: 100% 157/157 [00:19<00:00,  8.20it/s]\n",
      "Epoch: 9, AUROC is: 0.9560291111111111\n",
      "Train...: 100% 79/79 [01:07<00:00,  1.17it/s]\n",
      "Epoch: 10, Loss: 3.2759962043762205\n",
      "Train set feature extracting: 100% 79/79 [00:09<00:00,  8.16it/s]\n",
      "Test set feature extracting: 100% 157/157 [00:17<00:00,  8.80it/s]\n",
      "Epoch: 10, AUROC is: 0.9564313888888889\n",
      "Train...: 100% 79/79 [01:06<00:00,  1.18it/s]\n",
      "Epoch: 11, Loss: 3.2777525968551635\n",
      "Train set feature extracting: 100% 79/79 [00:09<00:00,  8.20it/s]\n",
      "Test set feature extracting: 100% 157/157 [00:18<00:00,  8.70it/s]\n",
      "Epoch: 11, AUROC is: 0.9567668333333332\n",
      "Train...: 100% 79/79 [01:08<00:00,  1.15it/s]\n",
      "Epoch: 12, Loss: 3.2878515266418455\n",
      "Train set feature extracting: 100% 79/79 [00:09<00:00,  8.77it/s]\n",
      "Test set feature extracting: 100% 157/157 [00:18<00:00,  8.46it/s]\n",
      "Epoch: 12, AUROC is: 0.9572584444444445\n",
      "Train...: 100% 79/79 [01:07<00:00,  1.17it/s]\n",
      "Epoch: 13, Loss: 3.244059230804443\n",
      "Train set feature extracting: 100% 79/79 [00:10<00:00,  7.89it/s]\n",
      "Test set feature extracting: 100% 157/157 [00:18<00:00,  8.68it/s]\n",
      "Epoch: 13, AUROC is: 0.9574533333333333\n",
      "Train...: 100% 79/79 [01:07<00:00,  1.17it/s]\n",
      "Epoch: 14, Loss: 3.2604661575317384\n",
      "Train set feature extracting: 100% 79/79 [00:09<00:00,  8.63it/s]\n",
      "Test set feature extracting: 100% 157/157 [00:18<00:00,  8.48it/s]\n",
      "Epoch: 14, AUROC is: 0.9577986666666667\n",
      "Train...: 100% 79/79 [01:08<00:00,  1.16it/s]\n",
      "Epoch: 15, Loss: 3.2404276615142824\n",
      "Train set feature extracting: 100% 79/79 [00:09<00:00,  8.17it/s]\n",
      "Test set feature extracting: 100% 157/157 [00:17<00:00,  8.72it/s]\n",
      "Epoch: 15, AUROC is: 0.9580394444444444\n",
      "Train...: 100% 79/79 [01:07<00:00,  1.17it/s]\n",
      "Epoch: 16, Loss: 3.232503706741333\n",
      "Train set feature extracting: 100% 79/79 [00:09<00:00,  8.65it/s]\n",
      "Test set feature extracting: 100% 157/157 [00:18<00:00,  8.52it/s]\n",
      "Epoch: 16, AUROC is: 0.9582447222222222\n",
      "Train...: 100% 79/79 [01:08<00:00,  1.16it/s]\n",
      "Epoch: 17, Loss: 3.2392725677490235\n",
      "Train set feature extracting: 100% 79/79 [00:09<00:00,  8.16it/s]\n",
      "Test set feature extracting: 100% 157/157 [00:17<00:00,  8.84it/s]\n",
      "Epoch: 17, AUROC is: 0.9584248888888889\n",
      "Train...: 100% 79/79 [01:08<00:00,  1.15it/s]\n",
      "Epoch: 18, Loss: 3.2294508895874023\n",
      "Train set feature extracting: 100% 79/79 [00:09<00:00,  8.47it/s]\n",
      "Test set feature extracting: 100% 157/157 [00:18<00:00,  8.71it/s]\n",
      "Epoch: 18, AUROC is: 0.9585517777777778\n",
      "Train...: 100% 79/79 [01:07<00:00,  1.18it/s]\n",
      "Epoch: 19, Loss: 3.2313364721298217\n",
      "Train set feature extracting: 100% 79/79 [00:09<00:00,  8.21it/s]\n",
      "Test set feature extracting: 100% 157/157 [00:18<00:00,  8.66it/s]\n",
      "Epoch: 19, AUROC is: 0.9586302222222222\n",
      "Train...: 100% 79/79 [01:07<00:00,  1.16it/s]\n",
      "Epoch: 20, Loss: 3.2104684703826902\n",
      "Train set feature extracting: 100% 79/79 [00:08<00:00,  9.12it/s]\n",
      "Test set feature extracting: 100% 157/157 [00:18<00:00,  8.32it/s]\n",
      "Epoch: 20, AUROC is: 0.9587746666666666\n"
     ]
    }
   ],
   "source": [
    "!python main.py --dataset=cifar10 --label=0 --backbone=18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8tfSV8NuXB3y"
   },
   "source": [
    "*  we need to seperate the dataset for different labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6jCemQ02VYSv",
    "outputId": "10a1ae66-ce72-4ee6-b42b-5d84a33f541d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ./data/cifar-100-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 169001437/169001437 [00:13<00:00, 12675639.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/cifar-100-python.tar.gz to ./data\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "# Define a transform to normalize the data\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Download and load the CIFAR-10 dataset\n",
    "testset = torchvision.datasets.CIFAR100(root='./data', train=False, download=True, transform=transform_test)\n",
    "test_loader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=1)\n",
    "\n",
    "# Create empty lists to hold datasets for each class\n",
    "airplane_dataset = []\n",
    "automobile_dataset = []\n",
    "bird_dataset = []\n",
    "cat_dataset = []\n",
    "deer_dataset = []\n",
    "dog_dataset = []\n",
    "frog_dataset = []\n",
    "horse_dataset = []\n",
    "ship_dataset = []\n",
    "truck_dataset = []\n",
    "\n",
    "# Loop over the entire training set and add each image to its respective class dataset\n",
    "for batch_idx, (data, target) in enumerate(test_loader):\n",
    "    for i in range(len(data)):\n",
    "        image, label = data[i], target[i]\n",
    "        if label == 0:\n",
    "            airplane_dataset.append((image, label))\n",
    "        elif label == 1:\n",
    "            automobile_dataset.append((image, label))\n",
    "        elif label == 2:\n",
    "            bird_dataset.append((image, label))\n",
    "        elif label == 3:\n",
    "            cat_dataset.append((image, label))\n",
    "        elif label == 4:\n",
    "            deer_dataset.append((image, label))\n",
    "        elif label == 5:\n",
    "            dog_dataset.append((image, label))\n",
    "        elif label == 6:\n",
    "            frog_dataset.append((image, label))\n",
    "        elif label == 7:\n",
    "            horse_dataset.append((image, label))\n",
    "        elif label == 8:\n",
    "            ship_dataset.append((image, label))\n",
    "        elif label == 9:\n",
    "            truck_dataset.append((image, label))\n",
    "\n",
    "# Create separate data loaders for each class dataset\n",
    "airplane_loader = torch.utils.data.DataLoader(airplane_dataset, batch_size=64, shuffle=True)\n",
    "automobile_loader = torch.utils.data.DataLoader(automobile_dataset, batch_size=64, shuffle=True)\n",
    "bird_loader = torch.utils.data.DataLoader(bird_dataset, batch_size=64, shuffle=True)\n",
    "cat_loader = torch.utils.data.DataLoader(cat_dataset, batch_size=64, shuffle=True)\n",
    "deer_loader = torch.utils.data.DataLoader(deer_dataset, batch_size=64, shuffle=True)\n",
    "dog_loader = torch.utils.data.DataLoader(dog_dataset, batch_size=64, shuffle=True)\n",
    "frog_loader = torch.utils.data.DataLoader(frog_dataset, batch_size=64, shuffle=True)\n",
    "horse_loader = torch.utils.data.DataLoader(horse_dataset, batch_size=64, shuffle=True)\n",
    "ship_loader = torch.utils.data.DataLoader(ship_dataset, batch_size=64, shuffle=True)\n",
    "truck_loader = torch.utils.data.DataLoader(truck_dataset, batch_size=64, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A3FBI-D-XJ96"
   },
   "source": [
    "*  we apply auto attack to dataset with our desired label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4oYkMGkMVc5H",
    "outputId": "f644fc03-9094-48a7-c69e-f3a8ebd37473"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting git+https://github.com/fra31/auto-attack\n",
      "  Cloning https://github.com/fra31/auto-attack to /tmp/pip-req-build-uxxiwgyq\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/fra31/auto-attack /tmp/pip-req-build-uxxiwgyq\n",
      "  Resolved https://github.com/fra31/auto-attack to commit a39220048b3c9f2cca9a4d3a54604793c68eca7e\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "setting parameters for standard version\n",
      "using standard version including apgd-ce, apgd-t, fab-t, square.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "PYDEV DEBUGGER WARNING:\n",
      "sys.settrace() should not be used when the debugger is being used.\n",
      "This may cause the debugger to stop working correctly.\n",
      "If this is needed, please check: \n",
      "http://pydev.blogspot.com/2007/06/why-cant-pydev-debugger-work-with.html\n",
      "to see how to restore the debug tracing back correctly.\n",
      "Call Location:\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/autoattack/checks.py\", line 100, in check_dynamic\n",
      "    sys.settrace(tracefunc)\n",
      "\n",
      "\n",
      "PYDEV DEBUGGER WARNING:\n",
      "sys.settrace() should not be used when the debugger is being used.\n",
      "This may cause the debugger to stop working correctly.\n",
      "If this is needed, please check: \n",
      "http://pydev.blogspot.com/2007/06/why-cant-pydev-debugger-work-with.html\n",
      "to see how to restore the debug tracing back correctly.\n",
      "Call Location:\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/autoattack/checks.py\", line 102, in check_dynamic\n",
      "    sys.settrace(None)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial accuracy: 8.00%\n",
      "apgd-ce - 1/1 - 8 out of 8 successfully perturbed\n",
      "robust accuracy after APGD-CE: 0.00% (total time 0.0 s)\n",
      "max Linf perturbation: 0.00000, nan in tensor: 0, max: 1.00000, min: 0.00000\n",
      "robust accuracy: 0.00%\n"
     ]
    }
   ],
   "source": [
    "class AdversarialDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        label = self.labels[idx]\n",
    "        return sample, label\n",
    "\n",
    "l = [x for (x, y) in airplane_loader]\n",
    "x_test = torch.cat(l, 0)\n",
    "l = [y for (x, y) in airplane_loader]\n",
    "y_test = torch.cat(l, 0)\n",
    "\n",
    "!pip install git+https://github.com/fra31/auto-attack\n",
    "from autoattack import AutoAttack\n",
    "adversary_Linf = AutoAttack(net, norm='Linf', eps=0.031, version='standard')\n",
    "adv_complete_Linf = adversary_Linf.run_standard_evaluation(x_test, y_test,bs=64)   \n",
    "\n",
    "adv_dataset_Linf = AdversarialDataset(adv_complete_Linf, y_test)\n",
    "adv_loader_Linf = torch.utils.data.DataLoader(adv_dataset_Linf, batch_size=100, shuffle=False, num_workers=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "veVGqJacXWO_"
   },
   "source": [
    "*   to make the dataset for evaluation we mix the standard dataset with auto attack dataset and then feed it to our anomaly detection model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YjNhFnqFVksH",
    "outputId": "2c615369-347a-45b0-a844-b7162d82f5b2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    }
   ],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, images, labels):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        image = self.images[index]\n",
    "        label = self.labels[index]\n",
    "        return torch.from_numpy(image), torch.tensor(label)\n",
    "\n",
    "# Load datasets\n",
    "dataset1 = x_test\n",
    "dataset2 = adv_complete_Linf\n",
    "\n",
    "# Create labels\n",
    "labels1 = np.ones(dataset1.shape[0])\n",
    "labels2 = np.zeros(dataset2.shape[0])\n",
    "\n",
    "# Concatenate datasets\n",
    "concatenated_dataset = np.concatenate((dataset1, dataset2), axis=0)\n",
    "concatenated_labels = np.concatenate((labels1, labels2), axis=0)\n",
    "\n",
    "# Shuffle dataset\n",
    "permutation = np.random.permutation(concatenated_dataset.shape[0])\n",
    "concatenated_dataset = concatenated_dataset[permutation]\n",
    "concatenated_labels = concatenated_labels[permutation]\n",
    "\n",
    "# Create dataset and dataloader\n",
    "custom_dataset = CustomDataset(concatenated_dataset, concatenated_labels)\n",
    "batch_size = 32\n",
    "num_workers = 4\n",
    "custom_dataloader = torch.utils.data.DataLoader(custom_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bufP1SJ5VlSn",
    "outputId": "28f7855b-b5e2-4716-b7f3-5ebd6ecadc84"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from sklearn.metrics import roc_auc_score\n",
    "def get_score(model, device, train_loader, test_loader):\n",
    "    train_feature_space = []\n",
    "    with torch.no_grad():\n",
    "        for (imgs, _) in tqdm(train_loader, desc='Train set feature extracting'):\n",
    "            imgs = imgs.to(device)\n",
    "            features = model(imgs)\n",
    "            train_feature_space.append(features)\n",
    "        train_feature_space = torch.cat(train_feature_space, dim=0).contiguous().cpu().numpy()\n",
    "    test_feature_space = []\n",
    "    test_labels = []\n",
    "    with torch.no_grad():\n",
    "        for (imgs, labels) in tqdm(test_loader, desc='Test set feature extracting'):\n",
    "            imgs = imgs.to(device)\n",
    "            features = model(imgs)\n",
    "            test_feature_space.append(features)\n",
    "            test_labels.append(labels)\n",
    "        test_feature_space = torch.cat(test_feature_space, dim=0).contiguous().cpu().numpy()\n",
    "        test_labels = torch.cat(test_labels, dim=0).cpu().numpy()\n",
    "\n",
    "    distances = utils.knn_score(train_feature_space, test_feature_space)\n",
    "\n",
    "    auc = roc_auc_score(test_labels, distances)\n",
    "\n",
    "    return auc, train_feature_space\n",
    "import utils\n",
    "model = utils.Model(backbone=18)\n",
    "model = model.to(device)\n",
    "def get_loaders(dataset, label_class, batch_size, backbone):\n",
    "    if dataset == \"cifar10\":\n",
    "        ds = custom_dataloader\n",
    "        transform = transform_color if backbone == 152 else transform_resnet18\n",
    "        coarse = {}\n",
    "        trainset = ds(root='data', train=True, download=True, transform=transform, **coarse)\n",
    "        testset = ds(root='data', train=False, download=True, transform=transform, **coarse)\n",
    "        trainset_1 = ds(root='data', train=True, download=True, transform=Transform(), **coarse)\n",
    "        idx = np.array(trainset.targets) == label_class\n",
    "        testset.targets = [int(t != label_class) for t in testset.targets]\n",
    "        trainset.data = trainset.data[idx]\n",
    "        trainset.targets = [trainset.targets[i] for i, flag in enumerate(idx, 0) if flag]\n",
    "        trainset_1.data = trainset_1.data[idx]\n",
    "        trainset_1.targets = [trainset_1.targets[i] for i, flag in enumerate(idx, 0) if flag]\n",
    "        train_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2,\n",
    "                                                   drop_last=False)\n",
    "        test_loader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2,\n",
    "                                                  drop_last=False)\n",
    "        return train_loader, test_loader, torch.utils.data.DataLoader(trainset_1, batch_size=batch_size,\n",
    "                                                                      shuffle=True, num_workers=2, drop_last=False)\n",
    "    else:\n",
    "        print('Unsupported Dataset')\n",
    "        exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "brlpLLDnPpsK",
    "outputId": "1e82fe35-5f9e-49df-d58f-92c67a9ddc1e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train set feature extracting: 100%|██████████| 157/157 [00:11<00:00, 13.54it/s]\n",
      "Test set feature extracting: 100%|██████████| 313/313 [00:19<00:00, 15.80it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8519885"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader, test_loader, train_loader_1 = utils.get_loaders(dataset=\"cifar10\", label_class=7, batch_size=32, backbone=18)\n",
    "a= get_score(model, device, train_loader, test_loader)\n",
    "a[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LNm-MaHxVGyn"
   },
   "source": [
    "For Label 0=airplane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qdYE_eDEVCtS"
   },
   "outputs": [],
   "source": [
    "!python main.py --dataset=cifar10 --label=0 --backbone=18\n",
    "class AdversarialDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        label = self.labels[idx]\n",
    "        return sample, label\n",
    "\n",
    "l = [x for (x, y) in airplane_loader]\n",
    "x_test = torch.cat(l, 0)\n",
    "l = [y for (x, y) in airplane_loader]\n",
    "y_test = torch.cat(l, 0)\n",
    "\n",
    "!pip install git+https://github.com/fra31/auto-attack\n",
    "from autoattack import AutoAttack\n",
    "adversary_Linf = AutoAttack(net, norm='Linf', eps=0.031, version='standard')\n",
    "adv_complete_Linf = adversary_Linf.run_standard_evaluation(x_test, y_test,bs=64)   \n",
    "\n",
    "adv_dataset_Linf = AdversarialDataset(adv_complete_Linf, y_test)\n",
    "adv_loader_Linf = torch.utils.data.DataLoader(adv_dataset_Linf, batch_size=100, shuffle=False, num_workers=2)\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, images, labels):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        image = self.images[index]\n",
    "        label = self.labels[index]\n",
    "        return torch.from_numpy(image), torch.tensor(label)\n",
    "\n",
    "# Load datasets\n",
    "dataset1 = x_test\n",
    "dataset2 = adv_complete_Linf\n",
    "\n",
    "# Create labels\n",
    "labels1 = np.ones(dataset1.shape[0])\n",
    "labels2 = np.zeros(dataset2.shape[0])\n",
    "\n",
    "# Concatenate datasets\n",
    "concatenated_dataset = np.concatenate((dataset1, dataset2), axis=0)\n",
    "concatenated_labels = np.concatenate((labels1, labels2), axis=0)\n",
    "\n",
    "# Shuffle dataset\n",
    "permutation = np.random.permutation(concatenated_dataset.shape[0])\n",
    "concatenated_dataset = concatenated_dataset[permutation]\n",
    "concatenated_labels = concatenated_labels[permutation]\n",
    "\n",
    "# Create dataset and dataloader\n",
    "custom_dataset = CustomDataset(concatenated_dataset, concatenated_labels)\n",
    "batch_size = 32\n",
    "num_workers = 4\n",
    "custom_dataloader = torch.utils.data.DataLoader(custom_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "import utils\n",
    "model = utils.Model(backbone=18)\n",
    "model = model.to(device)\n",
    "train_loader, test_loader, train_loader_1 = utils.get_loaders(dataset=\"cifar10\", label_class=7, batch_size=1000, backbone=18)\n",
    "a= get_score(model, device, train_loader, test_loader)\n",
    "a[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EbBXl3M-VIGL"
   },
   "source": [
    "For Label 1=automobile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1EWsragcVLuh"
   },
   "outputs": [],
   "source": [
    "#aslish ruie 152 bud\n",
    "!python main.py --dataset=cifar10 --label=1 --backbone=18\n",
    "class AdversarialDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        label = self.labels[idx]\n",
    "        return sample, label\n",
    "\n",
    "l = [x for (x, y) in automobile_loader]\n",
    "x_test = torch.cat(l, 0)\n",
    "l = [y for (x, y) in automobile_loader]\n",
    "y_test = torch.cat(l, 0)\n",
    "\n",
    "!pip install git+https://github.com/fra31/auto-attack\n",
    "from autoattack import AutoAttack\n",
    "adversary_Linf = AutoAttack(net, norm='Linf', eps=0.031, version='standard')\n",
    "adv_complete_Linf = adversary_Linf.run_standard_evaluation(x_test, y_test,bs=64)   \n",
    "\n",
    "adv_dataset_Linf = AdversarialDataset(adv_complete_Linf, y_test)\n",
    "adv_loader_Linf = torch.utils.data.DataLoader(adv_dataset_Linf, batch_size=100, shuffle=False, num_workers=2)\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, images, labels):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        image = self.images[index]\n",
    "        label = self.labels[index]\n",
    "        return torch.from_numpy(image), torch.tensor(label)\n",
    "\n",
    "# Load datasets\n",
    "dataset1 = x_test\n",
    "dataset2 = adv_complete_Linf\n",
    "\n",
    "# Create labels\n",
    "labels1 = np.ones(dataset1.shape[0])\n",
    "labels2 = np.zeros(dataset2.shape[0])\n",
    "\n",
    "# Concatenate datasets\n",
    "concatenated_dataset = np.concatenate((dataset1, dataset2), axis=0)\n",
    "concatenated_labels = np.concatenate((labels1, labels2), axis=0)\n",
    "\n",
    "# Shuffle dataset\n",
    "permutation = np.random.permutation(concatenated_dataset.shape[0])\n",
    "concatenated_dataset = concatenated_dataset[permutation]\n",
    "concatenated_labels = concatenated_labels[permutation]\n",
    "\n",
    "# Create dataset and dataloader\n",
    "custom_dataset = CustomDataset(concatenated_dataset, concatenated_labels)\n",
    "batch_size = 32\n",
    "num_workers = 4\n",
    "custom_dataloader = torch.utils.data.DataLoader(custom_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "import utils\n",
    "model = utils.Model(backbone=18)\n",
    "model = model.to(device)\n",
    "train_loader, test_loader, train_loader_1 = utils.get_loaders(dataset=\"cifar10\", label_class=7, batch_size=1000, backbone=18)\n",
    "a= get_score(model, device, train_loader, test_loader)\n",
    "a[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LRlqZY15VOIl"
   },
   "source": [
    "For Label 2= Bird"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3xQZHKILVPte"
   },
   "outputs": [],
   "source": [
    "#aslish ruie 152 bud\n",
    "!python main.py --dataset=cifar10 --label=2 --backbone=18\n",
    "class AdversarialDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        label = self.labels[idx]\n",
    "        return sample, label\n",
    "\n",
    "l = [x for (x, y) in bird_loader]\n",
    "x_test = torch.cat(l, 0)\n",
    "l = [y for (x, y) in bird_loader]\n",
    "y_test = torch.cat(l, 0)\n",
    "\n",
    "!pip install git+https://github.com/fra31/auto-attack\n",
    "from autoattack import AutoAttack\n",
    "adversary_Linf = AutoAttack(net, norm='Linf', eps=0.031, version='standard')\n",
    "adv_complete_Linf = adversary_Linf.run_standard_evaluation(x_test, y_test,bs=64)   \n",
    "\n",
    "adv_dataset_Linf = AdversarialDataset(adv_complete_Linf, y_test)\n",
    "adv_loader_Linf = torch.utils.data.DataLoader(adv_dataset_Linf, batch_size=100, shuffle=False, num_workers=2)\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, images, labels):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        image = self.images[index]\n",
    "        label = self.labels[index]\n",
    "        return torch.from_numpy(image), torch.tensor(label)\n",
    "\n",
    "# Load datasets\n",
    "dataset1 = x_test\n",
    "dataset2 = adv_complete_Linf\n",
    "\n",
    "# Create labels\n",
    "labels1 = np.ones(dataset1.shape[0])\n",
    "labels2 = np.zeros(dataset2.shape[0])\n",
    "\n",
    "# Concatenate datasets\n",
    "concatenated_dataset = np.concatenate((dataset1, dataset2), axis=0)\n",
    "concatenated_labels = np.concatenate((labels1, labels2), axis=0)\n",
    "\n",
    "# Shuffle dataset\n",
    "permutation = np.random.permutation(concatenated_dataset.shape[0])\n",
    "concatenated_dataset = concatenated_dataset[permutation]\n",
    "concatenated_labels = concatenated_labels[permutation]\n",
    "\n",
    "# Create dataset and dataloader\n",
    "custom_dataset = CustomDataset(concatenated_dataset, concatenated_labels)\n",
    "batch_size = 32\n",
    "num_workers = 4\n",
    "custom_dataloader = torch.utils.data.DataLoader(custom_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "import utils\n",
    "model = utils.Model(backbone=18)\n",
    "model = model.to(device)\n",
    "train_loader, test_loader, train_loader_1 = utils.get_loaders(dataset=\"cifar10\", label_class=7, batch_size=1000, backbone=18)\n",
    "a= get_score(model, device, train_loader, test_loader)\n",
    "a[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zX6k00jaVRsX"
   },
   "source": [
    "For Label 3= cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FrLJ6qBKVdyj"
   },
   "outputs": [],
   "source": [
    "#aslish ruie 152 bud\n",
    "!python main.py --dataset=cifar10 --label=3 --backbone=18\n",
    "class AdversarialDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        label = self.labels[idx]\n",
    "        return sample, label\n",
    "\n",
    "l = [x for (x, y) in cat_loader]\n",
    "x_test = torch.cat(l, 0)\n",
    "l = [y for (x, y) in cat_loader]\n",
    "y_test = torch.cat(l, 0)\n",
    "\n",
    "!pip install git+https://github.com/fra31/auto-attack\n",
    "from autoattack import AutoAttack\n",
    "adversary_Linf = AutoAttack(net, norm='Linf', eps=0.031, version='standard')\n",
    "adv_complete_Linf = adversary_Linf.run_standard_evaluation(x_test, y_test,bs=64)   \n",
    "\n",
    "adv_dataset_Linf = AdversarialDataset(adv_complete_Linf, y_test)\n",
    "adv_loader_Linf = torch.utils.data.DataLoader(adv_dataset_Linf, batch_size=100, shuffle=False, num_workers=2)\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, images, labels):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        image = self.images[index]\n",
    "        label = self.labels[index]\n",
    "        return torch.from_numpy(image), torch.tensor(label)\n",
    "\n",
    "# Load datasets\n",
    "dataset1 = x_test\n",
    "dataset2 = adv_complete_Linf\n",
    "\n",
    "# Create labels\n",
    "labels1 = np.ones(dataset1.shape[0])\n",
    "labels2 = np.zeros(dataset2.shape[0])\n",
    "\n",
    "# Concatenate datasets\n",
    "concatenated_dataset = np.concatenate((dataset1, dataset2), axis=0)\n",
    "concatenated_labels = np.concatenate((labels1, labels2), axis=0)\n",
    "\n",
    "# Shuffle dataset\n",
    "permutation = np.random.permutation(concatenated_dataset.shape[0])\n",
    "concatenated_dataset = concatenated_dataset[permutation]\n",
    "concatenated_labels = concatenated_labels[permutation]\n",
    "\n",
    "# Create dataset and dataloader\n",
    "custom_dataset = CustomDataset(concatenated_dataset, concatenated_labels)\n",
    "batch_size = 32\n",
    "num_workers = 4\n",
    "custom_dataloader = torch.utils.data.DataLoader(custom_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "import utils\n",
    "model = utils.Model(backbone=18)\n",
    "model = model.to(device)\n",
    "train_loader, test_loader, train_loader_1 = utils.get_loaders(dataset=\"cifar10\", label_class=7, batch_size=1000, backbone=18)\n",
    "a= get_score(model, device, train_loader, test_loader)\n",
    "a[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o7DavWMmVnXf"
   },
   "source": [
    "For Label 4= deer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qdRNdxzHVkts"
   },
   "outputs": [],
   "source": [
    "#aslish ruie 152 bud\n",
    "!python main.py --dataset=cifar10 --label=4 --backbone=18\n",
    "class AdversarialDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        label = self.labels[idx]\n",
    "        return sample, label\n",
    "\n",
    "l = [x for (x, y) in deer_loader]\n",
    "x_test = torch.cat(l, 0)\n",
    "l = [y for (x, y) in deer_loader]\n",
    "y_test = torch.cat(l, 0)\n",
    "\n",
    "!pip install git+https://github.com/fra31/auto-attack\n",
    "from autoattack import AutoAttack\n",
    "adversary_Linf = AutoAttack(net, norm='Linf', eps=0.031, version='standard')\n",
    "adv_complete_Linf = adversary_Linf.run_standard_evaluation(x_test, y_test,bs=64)   \n",
    "\n",
    "adv_dataset_Linf = AdversarialDataset(adv_complete_Linf, y_test)\n",
    "adv_loader_Linf = torch.utils.data.DataLoader(adv_dataset_Linf, batch_size=100, shuffle=False, num_workers=2)\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, images, labels):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        image = self.images[index]\n",
    "        label = self.labels[index]\n",
    "        return torch.from_numpy(image), torch.tensor(label)\n",
    "\n",
    "# Load datasets\n",
    "dataset1 = x_test\n",
    "dataset2 = adv_complete_Linf\n",
    "\n",
    "# Create labels\n",
    "labels1 = np.ones(dataset1.shape[0])\n",
    "labels2 = np.zeros(dataset2.shape[0])\n",
    "\n",
    "# Concatenate datasets\n",
    "concatenated_dataset = np.concatenate((dataset1, dataset2), axis=0)\n",
    "concatenated_labels = np.concatenate((labels1, labels2), axis=0)\n",
    "\n",
    "# Shuffle dataset\n",
    "permutation = np.random.permutation(concatenated_dataset.shape[0])\n",
    "concatenated_dataset = concatenated_dataset[permutation]\n",
    "concatenated_labels = concatenated_labels[permutation]\n",
    "\n",
    "# Create dataset and dataloader\n",
    "custom_dataset = CustomDataset(concatenated_dataset, concatenated_labels)\n",
    "batch_size = 32\n",
    "num_workers = 4\n",
    "custom_dataloader = torch.utils.data.DataLoader(custom_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "import utils\n",
    "model = utils.Model(backbone=18)\n",
    "model = model.to(device)\n",
    "train_loader, test_loader, train_loader_1 = utils.get_loaders(dataset=\"cifar10\", label_class=7, batch_size=1000, backbone=18)\n",
    "a= get_score(model, device, train_loader, test_loader)\n",
    "a[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eQcUt0bIVrgW"
   },
   "source": [
    "For Label 5= dog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qpceIOb5VpJ0"
   },
   "outputs": [],
   "source": [
    "#aslish ruie 152 bud\n",
    "!python main.py --dataset=cifar10 --label=5 --backbone=18\n",
    "class AdversarialDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        label = self.labels[idx]\n",
    "        return sample, label\n",
    "\n",
    "l = [x for (x, y) in dog_loader]\n",
    "x_test = torch.cat(l, 0)\n",
    "l = [y for (x, y) in dog_loader]\n",
    "y_test = torch.cat(l, 0)\n",
    "\n",
    "!pip install git+https://github.com/fra31/auto-attack\n",
    "from autoattack import AutoAttack\n",
    "adversary_Linf = AutoAttack(net, norm='Linf', eps=0.031, version='standard')\n",
    "adv_complete_Linf = adversary_Linf.run_standard_evaluation(x_test, y_test,bs=64)   \n",
    "\n",
    "adv_dataset_Linf = AdversarialDataset(adv_complete_Linf, y_test)\n",
    "adv_loader_Linf = torch.utils.data.DataLoader(adv_dataset_Linf, batch_size=100, shuffle=False, num_workers=2)\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, images, labels):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        image = self.images[index]\n",
    "        label = self.labels[index]\n",
    "        return torch.from_numpy(image), torch.tensor(label)\n",
    "\n",
    "# Load datasets\n",
    "dataset1 = x_test\n",
    "dataset2 = adv_complete_Linf\n",
    "\n",
    "# Create labels\n",
    "labels1 = np.ones(dataset1.shape[0])\n",
    "labels2 = np.zeros(dataset2.shape[0])\n",
    "\n",
    "# Concatenate datasets\n",
    "concatenated_dataset = np.concatenate((dataset1, dataset2), axis=0)\n",
    "concatenated_labels = np.concatenate((labels1, labels2), axis=0)\n",
    "\n",
    "# Shuffle dataset\n",
    "permutation = np.random.permutation(concatenated_dataset.shape[0])\n",
    "concatenated_dataset = concatenated_dataset[permutation]\n",
    "concatenated_labels = concatenated_labels[permutation]\n",
    "\n",
    "# Create dataset and dataloader\n",
    "custom_dataset = CustomDataset(concatenated_dataset, concatenated_labels)\n",
    "batch_size = 32\n",
    "num_workers = 4\n",
    "custom_dataloader = torch.utils.data.DataLoader(custom_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "import utils\n",
    "model = utils.Model(backbone=18)\n",
    "model = model.to(device)\n",
    "train_loader, test_loader, train_loader_1 = utils.get_loaders(dataset=\"cifar10\", label_class=7, batch_size=1000, backbone=18)\n",
    "a= get_score(model, device, train_loader, test_loader)\n",
    "a[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GSuCo87-Vwas"
   },
   "source": [
    "For Label 6= frog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QpEUee5mVt54"
   },
   "outputs": [],
   "source": [
    "#aslish ruie 152 bud\n",
    "!python main.py --dataset=cifar10 --label=6 --backbone=18\n",
    "class AdversarialDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        label = self.labels[idx]\n",
    "        return sample, label\n",
    "\n",
    "l = [x for (x, y) in frog_loader]\n",
    "x_test = torch.cat(l, 0)\n",
    "l = [y for (x, y) in frog_loader]\n",
    "y_test = torch.cat(l, 0)\n",
    "\n",
    "!pip install git+https://github.com/fra31/auto-attack\n",
    "from autoattack import AutoAttack\n",
    "adversary_Linf = AutoAttack(net, norm='Linf', eps=0.031, version='standard')\n",
    "adv_complete_Linf = adversary_Linf.run_standard_evaluation(x_test, y_test,bs=64)   \n",
    "\n",
    "adv_dataset_Linf = AdversarialDataset(adv_complete_Linf, y_test)\n",
    "adv_loader_Linf = torch.utils.data.DataLoader(adv_dataset_Linf, batch_size=100, shuffle=False, num_workers=2)\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, images, labels):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        image = self.images[index]\n",
    "        label = self.labels[index]\n",
    "        return torch.from_numpy(image), torch.tensor(label)\n",
    "\n",
    "# Load datasets\n",
    "dataset1 = x_test\n",
    "dataset2 = adv_complete_Linf\n",
    "\n",
    "# Create labels\n",
    "labels1 = np.ones(dataset1.shape[0])\n",
    "labels2 = np.zeros(dataset2.shape[0])\n",
    "\n",
    "# Concatenate datasets\n",
    "concatenated_dataset = np.concatenate((dataset1, dataset2), axis=0)\n",
    "concatenated_labels = np.concatenate((labels1, labels2), axis=0)\n",
    "\n",
    "# Shuffle dataset\n",
    "permutation = np.random.permutation(concatenated_dataset.shape[0])\n",
    "concatenated_dataset = concatenated_dataset[permutation]\n",
    "concatenated_labels = concatenated_labels[permutation]\n",
    "\n",
    "# Create dataset and dataloader\n",
    "custom_dataset = CustomDataset(concatenated_dataset, concatenated_labels)\n",
    "batch_size = 32\n",
    "num_workers = 4\n",
    "custom_dataloader = torch.utils.data.DataLoader(custom_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "import utils\n",
    "model = utils.Model(backbone=18)\n",
    "model = model.to(device)\n",
    "train_loader, test_loader, train_loader_1 = utils.get_loaders(dataset=\"cifar10\", label_class=7, batch_size=1000, backbone=18)\n",
    "a= get_score(model, device, train_loader, test_loader)\n",
    "a[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FF-EMrvwVyUu"
   },
   "source": [
    "For Label 8= ship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kdU0KD64V1tJ"
   },
   "outputs": [],
   "source": [
    "#aslish ruie 152 bud\n",
    "!python main.py --dataset=cifar10 --label=8 --backbone=18\n",
    "class AdversarialDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        label = self.labels[idx]\n",
    "        return sample, label\n",
    "\n",
    "l = [x for (x, y) in ship_loader]\n",
    "x_test = torch.cat(l, 0)\n",
    "l = [y for (x, y) in ship_loader]\n",
    "y_test = torch.cat(l, 0)\n",
    "\n",
    "!pip install git+https://github.com/fra31/auto-attack\n",
    "from autoattack import AutoAttack\n",
    "adversary_Linf = AutoAttack(net, norm='Linf', eps=0.031, version='standard')\n",
    "adv_complete_Linf = adversary_Linf.run_standard_evaluation(x_test, y_test,bs=64)   \n",
    "\n",
    "adv_dataset_Linf = AdversarialDataset(adv_complete_Linf, y_test)\n",
    "adv_loader_Linf = torch.utils.data.DataLoader(adv_dataset_Linf, batch_size=100, shuffle=False, num_workers=2)\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, images, labels):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        image = self.images[index]\n",
    "        label = self.labels[index]\n",
    "        return torch.from_numpy(image), torch.tensor(label)\n",
    "\n",
    "# Load datasets\n",
    "dataset1 = x_test\n",
    "dataset2 = adv_complete_Linf\n",
    "\n",
    "# Create labels\n",
    "labels1 = np.ones(dataset1.shape[0])\n",
    "labels2 = np.zeros(dataset2.shape[0])\n",
    "\n",
    "# Concatenate datasets\n",
    "concatenated_dataset = np.concatenate((dataset1, dataset2), axis=0)\n",
    "concatenated_labels = np.concatenate((labels1, labels2), axis=0)\n",
    "\n",
    "# Shuffle dataset\n",
    "permutation = np.random.permutation(concatenated_dataset.shape[0])\n",
    "concatenated_dataset = concatenated_dataset[permutation]\n",
    "concatenated_labels = concatenated_labels[permutation]\n",
    "\n",
    "# Create dataset and dataloader\n",
    "custom_dataset = CustomDataset(concatenated_dataset, concatenated_labels)\n",
    "batch_size = 32\n",
    "num_workers = 4\n",
    "custom_dataloader = torch.utils.data.DataLoader(custom_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "import utils\n",
    "model = utils.Model(backbone=18)\n",
    "model = model.to(device)\n",
    "train_loader, test_loader, train_loader_1 = utils.get_loaders(dataset=\"cifar10\", label_class=7, batch_size=1000, backbone=18)\n",
    "a= get_score(model, device, train_loader, test_loader)\n",
    "a[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4lSyE3gyV3dH"
   },
   "source": [
    "For Label 9= truck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qrvo4w6bV6m_"
   },
   "outputs": [],
   "source": [
    "#aslish ruie 152 bud\n",
    "!python main.py --dataset=cifar10 --label=9 --backbone=18\n",
    "class AdversarialDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        label = self.labels[idx]\n",
    "        return sample, label\n",
    "\n",
    "l = [x for (x, y) in truck_loader]\n",
    "x_test = torch.cat(l, 0)\n",
    "l = [y for (x, y) in truck_loader]\n",
    "y_test = torch.cat(l, 0)\n",
    "\n",
    "!pip install git+https://github.com/fra31/auto-attack\n",
    "from autoattack import AutoAttack\n",
    "adversary_Linf = AutoAttack(net, norm='Linf', eps=0.031, version='standard')\n",
    "adv_complete_Linf = adversary_Linf.run_standard_evaluation(x_test, y_test,bs=64)   \n",
    "\n",
    "adv_dataset_Linf = AdversarialDataset(adv_complete_Linf, y_test)\n",
    "adv_loader_Linf = torch.utils.data.DataLoader(adv_dataset_Linf, batch_size=100, shuffle=False, num_workers=2)\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, images, labels):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        image = self.images[index]\n",
    "        label = self.labels[index]\n",
    "        return torch.from_numpy(image), torch.tensor(label)\n",
    "\n",
    "# Load datasets\n",
    "dataset1 = x_test\n",
    "dataset2 = adv_complete_Linf\n",
    "\n",
    "# Create labels\n",
    "labels1 = np.ones(dataset1.shape[0])\n",
    "labels2 = np.zeros(dataset2.shape[0])\n",
    "\n",
    "# Concatenate datasets\n",
    "concatenated_dataset = np.concatenate((dataset1, dataset2), axis=0)\n",
    "concatenated_labels = np.concatenate((labels1, labels2), axis=0)\n",
    "\n",
    "# Shuffle dataset\n",
    "permutation = np.random.permutation(concatenated_dataset.shape[0])\n",
    "concatenated_dataset = concatenated_dataset[permutation]\n",
    "concatenated_labels = concatenated_labels[permutation]\n",
    "\n",
    "# Create dataset and dataloader\n",
    "custom_dataset = CustomDataset(concatenated_dataset, concatenated_labels)\n",
    "batch_size = 32\n",
    "num_workers = 4\n",
    "custom_dataloader = torch.utils.data.DataLoader(custom_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "import utils\n",
    "model = utils.Model(backbone=18)\n",
    "model = model.to(device)\n",
    "train_loader, test_loader, train_loader_1 = utils.get_loaders(dataset=\"cifar10\", label_class=7, batch_size=1000, backbone=18)\n",
    "a= get_score(model, device, train_loader, test_loader)\n",
    "a[0]"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
